{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Breakdown\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanics\n",
    "\n",
    "You should produce a Jupyter notebook in answer to this assignment.\n",
    "**You need to submit this notebook electronically as described below.**\n",
    "\n",
    "Place your notebook in a directory called `iamlans` and submit this directory using the submit command on a DICE machine. The format is:\n",
    "\n",
    "`submit iaml 4 iamlans`\n",
    "\n",
    "You can check the status of your submissions with the `show_submissions` command.\n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics MSc Degree Guide is that normally you will not be allowed to submit coursework late. See http://www.inf.ed.ac.uk/teaching/years/msc/courseguide10.html#exam for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you should NOT borrow actual text or code from other students. We ask that you provide a list of the people who you've had discussions with (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "1. In the following questions you are asked to run experiments using Python (version 2.7) and the following packages:\n",
    "    * Numpy\n",
    "    * Pandas\n",
    "    * Scikit-learn 0.17\n",
    "    * Matplotlib\n",
    "    * Seaborn\n",
    "\n",
    "2. Before you start make sure you have set up a vitual environment (or conda environment if you are working on your own machine) and the required packages installed. Instructions on how to set-up the working enviornment and install the required packages can be found in `01_Lab_1_Introduction`.\n",
    "\n",
    "3. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. **You are welcome to split your answer into multiple cells with intermediate printing.**\n",
    "\n",
    "4. The .csv files that you will be using are located at `./datasets` (the `datasets` directory is adjacent to this file).\n",
    "\n",
    "5. **IMPORTANT:** Keep your answers brief and concise. Most questions can be answered with 2-3 lines of explanation (excluding coding questions), unless stated otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this assignment you are asked to import all the packages and modules you will need. Include all required imports and execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda2/envs/iaml/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division #print_function\n",
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_predict, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = np.random.RandomState(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the datasets\n",
    "\n",
    "\n",
    "This assignment is based on two datasets:\n",
    "1. the 20 Newsgroups Dataset (you should recognise it from Assignment 1)\n",
    "2. the MNIST digits dataset\n",
    "\n",
    "### 20 Newsgroups\n",
    "\n",
    "For convenience, we repeat the description here. This dataset is a collection of approximately **20,000 newsgroup documents**, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are **very closely related** to each other (e.g. comp.sys.ibm.pc.hardware, comp.sys.mac.hardware), while others are **highly unrelated** (e.g misc.forsale, soc.religion.christian). \n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We will use documents from only **5 out of the 20 newsgroups**, which results in a 5-class problem. More specifically the 5 classes correspond to the following newsgroups: \n",
    "1. `alt.atheism`\n",
    "2. `comp.sys.ibm.pc.hardware`\n",
    "3. `comp.sys.mac.hardware`\n",
    "4. `rec.sport.baseball`\n",
    "5. `rec.sport.hockey `\n",
    "\n",
    "However, note here that classes **2-3** and **4-5** are rather closely related.\n",
    "\n",
    "**In contrast to Assignment 1**, we have opted to use **tf-idf** weights ([term frequency - inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf))\n",
    "for each word instead of the frequency counts. These weights represent the importance of a word to a\n",
    "document with respect to a collection of documents. The importance increases proportionally to the number\n",
    "of times a word appears in the document and decreases proportionally to the number of times the word\n",
    "appears in the whole corpus. \n",
    "\n",
    "Additionally we preprocess the data to include the **most frequent 1000 words** that are in **greater than 2 documents**, less than half of all documents, and that are not **[stop words](https://en.wikipedia.org/wiki/Stop_words)**.\n",
    "\n",
    "We will perform all this preprocessing for you.\n",
    "\n",
    "\n",
    "### MNIST\n",
    "This MNIST Dataset is a collection handwritten digits. The samples are partitioned (nearly) evenly across the **10 different digit classes {0, 1, . . . , 9}**. We use a preprocessed version for which the data are **$8 \\times 8$** pixel images containing one digit each. For further details on how the digits are preprocessed, see the sklearn documentation. The **images are grayscale**, with each pixel taking values in **{0, 1, . . . , 16}**, where 0 corresponds to black (weakest intensity) and 16 corresponds to white (strongest intensity). Therefore, the dataset is a **N Ã— 64** dimensional matrix where each dimension corresponds to a pixel from the image and N is the number of\n",
    "images. \n",
    "\n",
    "Again, to save you time, we perfom the import for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.        ,  0.        ],\n",
       "       [ 1.        ,  1.73205081],\n",
       "       [ 1.        , -1.73205081],\n",
       "       [-1.73205081,  3.        ],\n",
       "       [ 1.73205081,  3.        ],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "             [4, 0],\n",
    "             [1, math.sqrt(3)],\n",
    "             [1, -math.sqrt(3)],\n",
    "             [-math.sqrt(3), 3],\n",
    "             [math.sqrt(3), 3],\n",
    "             [0, 0]\n",
    "    ])\n",
    "\n",
    "print X.shape\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_p(mu,var):\n",
    "    return lambda x : math.exp(- math.pow(x - mu, 2) / (2*var) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, log=False):\n",
    "    Pgenuine = len(y[y==0]) / len(y)\n",
    "    if log:\n",
    "        print \"Pgenuine: \" + Pgenuine\n",
    "    Pintruder = len(y[y==1]) / len(y)\n",
    "\n",
    "    if log:\n",
    "        print \"Pintruder: \" + Pintruder\n",
    "        \n",
    "    mu_ls_genuine = np.mean(X[y==0, 0])\n",
    "    var_ls_genuine = np.var(X[y==0, 0])\n",
    "    #mu_ls_genuine, var_ls_genuine\n",
    "    mu_ls_intruder = np.mean(X[y==1, 0])\n",
    "    var_ls_intruder = np.var(X[y==1, 0])\n",
    "    #mu_ls_intruder, var_ls_intruder\n",
    "    mu_cd_genuine = np.mean(X[y==0, 1])\n",
    "    var_cd_genuine = np.var(X[y==0, 1])\n",
    "    #mu_cd_genuine, var_cd_genuine\n",
    "    mu_cd_intruder = np.mean(X[y==1, 1])\n",
    "    var_cd_intruder = np.var(X[y==1, 1])\n",
    "    #mu_cd_intruder, var_cd_intruder\n",
    "    \n",
    "    p_ls_genuine = get_p(mu_ls_genuine, var_ls_genuine)\n",
    "    p_ls_intruder = get_p(mu_ls_intruder, var_ls_intruder)\n",
    "    p_cd_genuine = get_p(mu_cd_genuine, var_cd_genuine)\n",
    "    p_cd_intruder = get_p(mu_cd_intruder, var_cd_intruder)\n",
    "    \n",
    "    p_intruder = lambda ls, cd : Pintruder * p_ls_intruder(ls) * p_cd_intruder(cd)\n",
    "    p_genuine = lambda ls, cd : Pgenuine * p_ls_genuine(ls) * p_cd_genuine(cd)\n",
    "    \n",
    "    return p_intruder, p_genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(ls, cd, log=False):\n",
    "    intruder = p_intruder(ls, cd)\n",
    "    genuine = p_genuine(ls, cd)\n",
    "    \n",
    "    if log:\n",
    "        print \"intruder: %f\" % intruder\n",
    "        print \"genuine: %f\" % genuine\n",
    "    \n",
    "    if genuine > intruder:\n",
    "        return 0\n",
    "    elif intruder > genuine:\n",
    "        return 1\n",
    "    else: return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intruder: 0.183940\n",
      "genuine: 0.183940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_intruder, p_genuine = train(X, y)\n",
    "test(0, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPreds(X, y):\n",
    "    p_intruder, p_genuine = train(X, y)\n",
    "    return np.array([test(x[0], x[1]) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  1,  1, -1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPreds(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0,  1,  1, -1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPreds(X[1:], y[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  1,  1, -1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPreds(np.delete(X, 1, axis=0), np.delete(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1, -1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPreds(np.delete(X, 2, axis=0), np.delete(y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  1, -1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPreds(np.delete(X, 3, axis=0), np.delete(y, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  1, -1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPreds(np.delete(X, 4, axis=0), np.delete(y, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPreds(np.delete(X, 5, axis=0), np.delete(y, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [iaml]",
   "language": "python",
   "name": "Python [iaml]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
